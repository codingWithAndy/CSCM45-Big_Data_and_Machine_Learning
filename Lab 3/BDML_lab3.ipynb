{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Linear Regression, Principal Component Analysis and Linear Discriminant Analysis\n",
    "In this lab we look at utilizing Linear Regression, PCA and LDA. We will work on putting together the skeleton code to create the functionality of the algorithms, applying them to several different tasks.\n",
    "\n",
    "For Linear Regression we will be loading in some 2D datapoints and fitting a linear regression model, before visualizing the resulting fit. \n",
    "\n",
    "For Principal Component Analysis and Linear Discriminant Analysis we will be analysing and reducing the feature space of the Wine dataset.\n",
    "\n",
    "As usual, we will then look at using pre-implemented functionality for these tasks by using sklearn.\n",
    "\n",
    "<b>Important note:</b> \n",
    "    Please do not edit the existing code snippets. Instead, add your functionality into the TODO sections. Read the entire skeleton structure first and think about how you should structure the code you are adding in carefully.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do your package imports here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1 - Linear Regression\n",
    "This task will load the example data from y_observations.npy and x_points.npy and fit a linear regression model to the data. We plot the development of the model training in each step. The following subtasks are:\n",
    "1. Plot the data.\n",
    "2. Select a number of iterations.\n",
    "3. Selection a learning rate.\n",
    "4. Plot the initial model prediction.\n",
    "5. Calculate the predicted y values.\n",
    "6. Update w0.\n",
    "7. Update w1.\n",
    "\n",
    "There are <b>11 TODOs</b> in this Task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the regression problem data\n",
    "x = np.load('x_points.npy')\n",
    "y = np.load('y_observations.npy')\n",
    "\n",
    "# Plot the observed data\n",
    "# TODO: Scatter plot the observed Y against the X axis.\n",
    "# TODO: Give figure suitable axes labels and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise starting parameters w0 and w1\n",
    "w0 = np.random.rand()\n",
    "w1 = np.random.rand()\n",
    "\n",
    "# Select training hyper-parameters: number of iterations and learning rate control parameter alpha\n",
    "n_iteration = # TODO: Select a number of iterations.\n",
    "alpha = # TODO: Select a value for alpha (between 0 and 1).\n",
    "\n",
    "# Plot initial linear regression estimate\n",
    "# TODO: Scatter plot of X vs. Y.\n",
    "# TODO: Plot our predictions over the top.\n",
    "# TODO: Give figure suitable axes labels and title.\n",
    "\n",
    "# Iterate\n",
    "for i_iteration in range(0, n_iteration):\n",
    "    \n",
    "    # Predicted Y values of X with current w0 and w1 parameters\n",
    "    y_fit = # TODO: predict y_fit = f(w0,w1,x) = wx+b\n",
    "    \n",
    "    # Calculate difference between observed data and model's prediction\n",
    "    y_diff = y_fit - y \n",
    "    \n",
    "    # Calculate partial derivative of mean squares error with respect to w0\n",
    "    pd_mse_w0 = np.sum(y_diff) / np.size(y)\n",
    "    \n",
    "    # Calculate partial derivative of mean squares error with respect to w1\n",
    "    pd_mse_w1 = np.sum(y_diff * x) / np.size(y)\n",
    "    \n",
    "    # Update w0\n",
    "    w0 = # TODO: Update w0 parameter. Hint: Lecture 4, slide 9.\n",
    "    \n",
    "    # Update w1\n",
    "    w1 = # TODO: Update w1 parameter. Hint: Lecture 4, slide 9.\n",
    "    \n",
    "    # Calculate MSE \n",
    "    mse = np.mean(((w0 + w1 * x) - y) ** 2)\n",
    "    \n",
    "    # Print progress\n",
    "    if (i_iteration % 50) == 0:\n",
    "        print(\"-> Iter: {0}, Update w0: {1:.2f}, Update w1: {2:.2f}, MSE: {3:.8f}\".format(i_iteration, w0, w1, mse))\n",
    "        plt.scatter(x, y)\n",
    "        plt.plot(x, w0 + w1 * x)\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.title('Updated Regression estimate')\n",
    "        plt.show(block=False)\n",
    "        \n",
    "print(\"Final model: w0 = {0:.2f}, w1 = {1:.2f}, MSE = {2:.8f}\".format(w0, w1, mse))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, w0 + w1 * x)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Final Regression estimate')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 3.2 Principal Component Analysis\n",
    "This task will load the example data from wineData.npy and wineLabels.npy and perform Principal Component Analysis on the data in order to reduce the feature dimensionality of the problem. We first divide the dataset into a training and testing set and perform the PCA projection on each set. The following subtasks are:\n",
    "1. Load the data and labels.\n",
    "2. Plot the original dataset.\n",
    "3. Divide the dataset into a testing and training set. \n",
    "4. Plot the two sets on the same figure. \n",
    "5. Mean-centre the training set for SVD.\n",
    "6. Plot the percentage variance explained by each component. \n",
    "7. Select a number of components to keep based on the explained variance.\n",
    "8. Create the projection matrix by slicing into Vt.\n",
    "9. Project the training data into the Principal Component space.\n",
    "10. Mean-centre and project the testing data into the Principal Component space.\n",
    "11. Plot the training and testing set in the Principal Component space.\n",
    "\n",
    "There are <b>24 TODOs</b> in this Task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the Wine dataset\n",
    "# TODO: load wine data and wine labels into x and y respectively.\n",
    "x =\n",
    "y =\n",
    "\n",
    "# Plot two feature dimensions against eachother\n",
    "# TODO: Plot two feature dimensions of the wine dataset against eachother, labelling the points with the class label.\n",
    "# TODO: Give figure suitable axes labels and title.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data points to divide into a training and testing set\n",
    "test_train_ratio = # TODO: Choose a sensible test:train ratio (between 0 and 1).\n",
    "test_indices = np.random.choice(np.arange(x.shape[0]), int(x.shape[0] * test_train_ratio), replace=False)\n",
    "train_indices = np.delete(np.arange(x.shape[0]), test_indices)\n",
    "\n",
    "# Divide the dataset into the two sets\n",
    "test_data = # TODO: Index into the data and extract the testing samples.\n",
    "test_labels = # TODO: Index into the labels and extract the testing labels.\n",
    "train_data = # TODO: Index into the data and extract the training samples.\n",
    "train_labels = # TODO: Index into the labels and extract the training labels.\n",
    "\n",
    "# Plot the training samples, then plot the testing samples over the top with different marker shapes\n",
    "# TODO: Plot the training set, label points with the class label.\n",
    "# TODO: In the same figure, plot the testing set and label points with the class label.\n",
    "# TODO: Make sure that you give a different marker to the training and testing sets.\n",
    "# TODO: Give figure suitable axes labels and title.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the training data\n",
    "mu = # TODO: Calculate the mean vector of the data.\n",
    "centred_train_data = # TODO: Mean-centre the training samples.\n",
    "U, S, Vt = np.linalg.svd(centred_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the variance explained by each of the principal components. This may help you to select a number of\n",
    "# principle components to keep\n",
    "percentage_variance_explained = S**2 / np.sum(S**2)\n",
    "\n",
    "# Plot the percentage variance explained by each of the principal component spaces\n",
    "# TODO: Plot the percentage variance explained as calculated above.\n",
    "# TODO: Give figure suitable axes labels and title.\n",
    "plt.figure()\n",
    "plt.plot(percentage_variance_explained)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage variance explained by each of the identified principal components')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of components to keep and create the projection matrix\n",
    "n_components = # TODO: Select a number of principal components to keep. Hint: Look at the figure above.\n",
    "projection_matrix =  # TODO: Slice into Vt to obtain projection matrix. Hint: Lecture 5, slide 22.\n",
    "\n",
    "# Project the training data into the Principal Component space\n",
    "projected_train_data = # TODO: Project the training data into the new PCA space. Hint: Lecture 5, slide 22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the testing data into the Principal Component space\n",
    "centred_test_data = # TODO: Mean-centre the testing data.\n",
    "\n",
    "# Project the data into principal component space\n",
    "projected_test_data = # TODO: Project the training data into the new PCA space. Hint: Lecture 5, slide 22.\n",
    "\n",
    "# Plot the training and testing data projected in the new Principal Component space\n",
    "# TODO: Plot the training set in the new principal component space.\n",
    "# TODO: In the same figure, plot the testing set in the new Principal Component space.\n",
    "# TODO: Give figure suitable axes labels and title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3 - Linear Discriminant Analysis\n",
    "This task we will again load the example data from wineData.npy and wineLabels.npy and now we will perform Linear Discriminant Analysis on the data in order to reduce the feature dimensionality of the problem. The following subtasks are:\n",
    "1. Load the data and labels.\n",
    "5. Calculate the mean of each class in order to compute the within- and between-class scatter matrices.\n",
    "6. Calculate and plot the percentage variance explained by each discriminant. \n",
    "7. Select a number of discriminants to keep based on the explained variance.\n",
    "9. Project the data into the linear discriminant space.\n",
    "11. Plot the data in the linear discriminant space.\n",
    "\n",
    "There are <b>8 TODOs</b> in this Task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wine data\n",
    "# TODO: load wine data and wine labels into x and y respectively.\n",
    "x =\n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some basic data properties\n",
    "n_class = np.max(y) + 1\n",
    "n_samples = x.shape[0]\n",
    "n_features = x.shape[1]\n",
    "\n",
    "# Calculate class sample counts\n",
    "n_samples_in_class = np.zeros(n_class)\n",
    "for i_class in range(n_class):\n",
    "    n_samples_in_class[i_class] = x[y==i_class, :].shape[0]\n",
    "\n",
    "# Calculate class means\n",
    "mu = np.mean(x, axis=0)\n",
    "mu_class = np.zeros((n_class, n_features))\n",
    "for i_class in range(n_class):\n",
    "    mu_class[i_class, :] = # TODO: Calculate mean vector of each class. Hint: Lecture 6, slide 9.\n",
    "\n",
    "# Calculate within-class scatter matrix\n",
    "Sw = np.zeros((n_features, n_features))\n",
    "for i_class in range(n_class):\n",
    "    Si = np.zeros((n_features, n_features))\n",
    "    cluster_data = x[y==i_class, :]\n",
    "    for i_data in range(cluster_data.shape[0]):\n",
    "        centred_datapoint = cluster_data[i_data:i_data+1, :] - mu_class[i_class:i_class+1, :]\n",
    "        Si += centred_datapoint.T @ centred_datapoint\n",
    "    Sw += Si\n",
    "    \n",
    "# Calculate between-class scatter matrix\n",
    "mu_class_mu = np.zeros((n_class, n_features))\n",
    "for i_class in range(n_class):\n",
    "    mu_class_mu[i_class, :] = mu_class[i_class, :] - mu\n",
    "    \n",
    "Sb = np.zeros((n_features, n_features))\n",
    "for i_class in range(n_class):\n",
    "    Sb += num_samples_in_class[i_class] * mu_class_mu[i_class:i_class+1, :].T @ mu_class_mu[i_class:i_class+1, :]\n",
    "\n",
    "# Solve eigen decomposition for inv(Sw)Sb\n",
    "eigvalues, eigvectors = np.linalg.eig(np.linalg.inv(Sw) @ Sb)\n",
    "\n",
    "# Eig doesn't guarantee a sorted output, so sort the eigenvectors based on the eigenvalues\n",
    "sorted_indices = np.argsort(eigvalues)[::-1]\n",
    "eigvalues = eigvalues[sorted_indices]\n",
    "eigvectors = eigvectors[:, sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the variance explained by each of the principal components\n",
    "# TODO: Calculate the percentage variance explained by each of the linear discriminant spaces.\n",
    "\n",
    "# Plot the percentage variance explained by each of the linear discriminant spaces\n",
    "# TODO: Plot the percentage variance explained as calculated above.\n",
    "# TODO: Give figure suitable axes labels and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the feature channels we want to keep based on the variance explained by the Linear Discriminant Analysis\n",
    "n_dimensions = # TODO: Select a number of discriminant spaces to keep.\n",
    "projection_matrix = eigvectors[:, 0:n_dimensions].real \n",
    "\n",
    "# Project observed data into the new subspace\n",
    "projected_data = x @ projection_matrix\n",
    "\n",
    "# Plot data on new linear discriminant\n",
    "# TODO: Plot the data projected onto the new linear discriminant space, label classes appropriately\n",
    "# TODO: Give figure suitable axes labels and title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SciKit-Learn\n",
    "In this section we will utilise SciKit-Learn and use the built-in API functionality to complete the tasks from above.\n",
    "\n",
    "We will again perform Linear Regression on the toy data, and Principal Component Analysis and Linear Discriminant Analysis on the wine dataset.\n",
    "\n",
    "We can use the <b>sklearn.linear_model.LinearRegression</b> object to fit a regression model to our toy data using the <i>fit</i> function.\n",
    "\n",
    "We can use the <b>sklearn.decomposition.PCA</b> object to analyse the observed data and project it to the new principle component space using the <i>fit_transform</i> function. \n",
    "\n",
    "We can use the <b>sklearn.discriminant_analysis</b> object to analyse the observed data and project it to the new new discriminant space using the <i>fit_transform</i> function. \n",
    "\n",
    "There are <b>17 TODOs</b> in this Task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Use sklearn for Linear Regression\n",
    "# TODO: Load the regression X and Y values as in Task 3.1.\n",
    "# TODO: Instantiate a LinearRegression model object.\n",
    "# TODO: Fit the model to the observations.\n",
    "# TODO: Print the intercept and coefficient of the model.\n",
    "# TODO: Predict the Y values of X so that we can observe our linear model.\n",
    "# TODO: Scatter plot the data and then plot the predicted Y values on top. \n",
    "# TODO: Give figure suitable axes labels and title.\n",
    "\n",
    "# Use sklearn for PCA\n",
    "# TODO: Load the wine dataset as in Task 3.2.\n",
    "# TODO: Instantiate a PCA model object with a number of components as chosen in Task 3.2.\n",
    "# TODO: Fit the model to the data and project it to the new space.\n",
    "# TODO: Plot the projected data points.\n",
    "# TODO: Give figure suitable axes labels and title.\n",
    "\n",
    "# Use sklearn for LDA\n",
    "# TODO: Load the wine dataset as in Task 3.3.\n",
    "# TODO: Instantiate a LinearDiscriminantAnalysis model object with a number of components as chosen in Task 3.3.\n",
    "# TODO: Fit the model to the data and project it to the new space.\n",
    "# TODO: Plot the projected data points.\n",
    "# TODO: Give figure suitable axes labels and title."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
